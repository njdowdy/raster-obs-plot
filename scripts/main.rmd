---
title: "North American Bee Data - Raster and Observation Plots"
author: "Dr. Nicolas J. Dowdy"
date: '2021-01-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(12345) # set seed for code reproducibility
# set resource locations
setwd("scripts/")
input_csv_location = "../input/specieslist.csv"
raster_location = "../input/extracted/range_data_rasters/"
shape_files_location = "../input/extracted/shape-files/"
image_location = "../images/"
```
![](`r paste(image_location, 'bees.jpg', sep='')`){width=50%}

## Summary of Input Data

The file "input/specieslist.csv" contains a list of 3685 North American species of Hymenoptera. For each taxon, this file contains:
  1. Family Name
  2. Genus Name
  3. Species Name
  4. "Name" (genus_species)
  5. Corresponding Number of Occurrence Records

Spatial polygons have been previously generated for each taxon and are composed of two files with extensions ".grd" and ".gri". The corresponding files for each taxon are stored in:

1. `../input/extracted/range_data_rasters/220km_family_genus_species_raster.grd`
2. `../input/extracted/range_data_rasters/220km_family_genus_species_raster.gri`

** Note: these are stored in a .7z archive in the repository and need to be extracted

## Code Requirements

Read in each range file, calculate the area of each polygon (see area() in *raster* package), add the calculated area to the csv document. 

* Loop over species
* Read in raster
* Measure area of range
* Write out in in new columns of "specieslist.csv":
  * Range size
  * Range size as a proportion of continental US area, plus Alaska and Hawaii
  * Total number of records needed (this would be the 6 mil potentially digitizable records)
    * Assume there are ~2 million digitized in this dataset and ~6 million undigitized, but potentially digitizable specimen records
    * How can we estimate the distribution of these 6 million records over these taxa?
      * Option 1: evenly distributed
        * Probably not realistic; assumes prior digitization has occurred in proportion to holdings
      * Option 2: differentially weighted based on some metric (e.g., range size, proportion of currently digitized records, etc)
        * Not clear what criteria would best capture the number of remaining undigitized specimens for all species
  * New records added (total records needed - current records)
* Based on number of "New Records Added", generate N random lat/long points within each species' range
* Export generated lat/longs into a csv file for each corresponding taxon
  * Artificial Observation ID, Family, Genus, Species, Latitude, Longitude

```{r echo=FALSE}
# import libraries for this chunk
library(DT) # for making nice tables
df <- as.data.frame(read.csv(input_csv_location)) # load data
datatable(df, extensions = "Scroller", width = 1000, options = list(scrollY = 200, scroller = TRUE, scrollX = 200, pageLength = 1)) # view datatable in rmarkdown
```

```{r echo=FALSE}
# load libraries for this chunk
library(raster)
library(rgdal)
library(rgeos)
library(dplyr)
library(progress) # creates progress bars (this chunk can take a long time)

#library(rasterVis) # this may not be needed

## read in the shape files and crop
usa=readOGR(paste(shape_files_location, "usa" ,sep=""))
globe=readOGR(paste(shape_files_location, "continents" ,sep=""))
NAm=globe[globe$CONTINENT=='North America',]
NAm=crop(NAm,extent(-165,-60,8,85))
usaWGS=spTransform(usa,CRS(proj4string(NAm)))

# generate a map for plotting
# note don't run this too much - it costs money through Google API key
#library(ggmap)
#register_google_key(key=./secret-keys/google-api-key.txt) # you must provide your own google api key
#map_all_nam <- get_map(location='united states', zoom=3, maptype = "satellite", source='google', color='color')

taxa <- unique(df$name) # get a list of unique taxa
grd_files <- list.files(raster_location, pattern = "\\.grd$") # generate a list of raster locations
name <- c() # allocate as a temporary list
area_range <- c() # allocate as a temporary list
area_prop_blocks_range <- c() # allocate as a temporary list
area_prop_USA_range <- c() # allocate as a temporary list
pb <- progress_bar$new(format = "[:bar] :current/:total (:percent); eta: :eta", total = length(taxa)) # format progress bar
pb$tick(0) # set start progress
print("Starting Loop. Go get some tea!")
## for loop could use some speed up from for_each package
for (taxon in taxa) { # for each taxon in the csv file:
  pb$tick()
  # taxon = "Epeoloides_pilosulus" # for testing
  # taxon = "Andrena_accepta" # for testing
  raster_file = grd_files[grep(taxon, grd_files)] # find the taxon raster file
  # place a conditional here in case raster is not found
  raster = raster(paste(raster_location, raster_file, sep="")) # load the raster
  raster_usawgs = mask(raster, usaWGS) # mask the raster to the USA shapefile
  raster_nam = mask(raster, NAm) # mask the raster to the North America shapefile
  plot(raster_nam) # plot the raster as sanity check
  df_area <- as.data.frame(raster_usawgs, xy=T) # store the range + USA data in data frame
  colnames(df_area) <- c("long", "lat", "presence_absence") # change the column names for easy referencing
  df_area$presence_absence <- as.factor(df_area$presence_absence) # change presence_absence column into factor variable for plotting
  df_area_restrict <- df_area[!is.na(df_area$presence_absence),] # generate data frame containing only non-NA presence/absence data (not totally necessary, but I prefer it)
  
  # produce a rough map of the distribution (resolution = 220km2)
  #library(ggplot2)
  #ggmap(map_all_nam) +
  #  geom_point(data = df_area_restrict, aes(x = long, y = lat, color = presence_absence), size = 2) +
  #  guides(fill=FALSE, alpha=FALSE, size=FALSE) +
  #  scale_colour_manual(values = c("black","red")) # for test taxon, how can there only be 39 records in specieslist.csv? The range map suggest at least 46.
  
  # calculate area
  area <- (count((df_area_restrict %>% filter(presence_absence == 1)))*220*220)$n # area of range (220km*220km resolution)
  # wikipedia data:
  # total area of all 50 states: 9,833,517 km2
  # '                ' contiguous US: 8,081,867 km2
  # total land area of all 50 state: 9,147,593 km2
  # '                ' contiguous US: 7,653,004 km2
  # calculated from raster: 13,019,600 km2
  area_prop_blocks <- area / (nrow(df_area_restrict)*220*220) # area of range relative to total area of USA (220km*220km resolution); this large resolution leads to large overestimate of area
  area_prop_USA <- area / 9833517 # area of range relative to reference area of USA (this will lead to slightly larger proportional range sizes due to ranges being at 220km*220km resolution; in other words, USA-restricted ranges are overestimated here due to block size)
  
  # store values for writing
  name <- c(name, taxon)
  area_range <- c(area_range, area)
  area_prop_blocks_range <- c(area_prop_blocks_range, area_prop_blocks)
  area_prop_USA_range <- c(area_prop_USA_range, area_prop_USA)
}
temp <- data.frame(name, area_range, area_prop_blocks_range, area_prop_USA_range) # combine temp lists
new_df <- left_join(df, temp, by="name") # join temp data frame with csv data frame
```

## Determine Potential Number of Undigitized Records 

* Get sum of all ranges ("total_range_sum")
* Allocate proportion of 8 million records for each species as:
    * ((area_range / total_range_sum) * 8000000) - number_existing_occurrence

```{r}
# new_df <- read.csv('../output/specieslist_out_2021-01-13.csv') # load this rather than repeat chunk above
total_range_sum <- sum(new_df$area_range, na.rm = TRUE) # sum area of all ranges
new_df <- new_df %>% mutate(record_prop = area_range / total_range_sum) # calculate proportion of each range relative to sum of all ranges
# new_df <- new_df %>% mutate(num_records_to_sample = (8000000 * record_prop) - n) # old method for calculate number of undigitized records
new_df <- new_df %>% mutate(num_records_to_sample = (6000000 * record_prop)) # calculate number of undigitized records
write.csv(new_df,sub(".csv", "_out.csv",sub("input","output",input_csv_location)), row.names = FALSE) # write out results
```

## Generate New Points

* For each taxon, generate new records at random lat/longs within the range

```{r echo=FALSE}
df <- as.data.frame(read.csv(sub(".csv", "_out.csv",sub("input","output",input_csv_location)))) # load previously output data
name <- c() # allocate as a temporary list
genus <- c() # allocate as a temporary list
species <- c() # allocate as a temporary list
new_long_list <- c() # allocate as a temporary list
new_lat_list <- c() # allocate as a temporary list
pb <- progress_bar$new(format = "[:bar] :current/:total (:percent); eta: :eta", total = length(taxa)) # format progress bar
pb$tick(0) # set start progress
print("Starting Loop. Go get some coffee!")
## for loop could use some speed up from for_each package
for (taxon in taxa) { # for each taxon in the csv file:
  # taxon = "Epeoloides_pilosulus" # for testing
  # unfortunately, we have to reload rasters:
  pb$tick(1) # increment progress
  raster_file = grd_files[grep(taxon, grd_files)] # find the taxon raster file
  raster = raster(paste(raster_location, raster_file, sep="")) # load the raster
  raster_usawgs = mask(raster, usaWGS) # mask the raster to the USA shapefile
  raster_nam = mask(raster, NAm) # mask the raster to the North America shapefile
  plot(raster_nam) # plot the raster as sanity check
  df_area <- as.data.frame(raster_usawgs, xy=T) # store the range + USA data in data frame
  colnames(df_area) <- c("long", "lat", "presence_absence") # change the column names for easy referencing
  df_area$presence_absence <- as.factor(df_area$presence_absence) # change presence_absence column into factor variable for plotting
  df_area_restrict <- df_area[!is.na(df_area$presence_absence),] # generate data frame containing only non-NA presence/absence data (not totally necessary, but I prefer it)
  ## generate random points
  # 220km resolution blocks are 2* x 2*
  range <- df_area_restrict[df_area_restrict$presence_absence==1,] # get blocks that are part of range
  lat_list = unique(range$lat) # generate a list of latitudes
  coordinates <- data.frame() # create a dataframe to store coordinate pairs in
  for (i in 1:length(lat_list)){ # for each latitude in range:
    temp <- range %>% filter(lat == lat_list[i]) # return only blocks from that latitude
    dtemp <- data.frame(temp$long, rep(lat_list[i], length(temp$long))) # create lat/long pairs for each block
    coordinates <- rbind(coordinates, dtemp) # store these pairs
  }
  names(coordinates) <- c("long", "lat") # change the column names for easy referencing
  new_points <- coordinates[sample(nrow(coordinates), round(pmax(df[df$name==taxon,]$num_records_to_sample,0)), replace = TRUE), ] # sample the coordinate blocks to create new lat/long points
  random_latlong <- function(x) { # create a function to apply random lat/long jitter
                         x + sample(seq(-1,1,0.01),1) 
                        # here we have chosen to add +/- 1 degree of lat and long 
                        # because block size is 2 degrees, this places a point randomly
                        # within each block, but not outside that block
                    }
  new_long <- t(as.data.frame(lapply(new_points$long, random_latlong))) # apply jitter to longitude
  new_lat  <- t(as.data.frame(lapply(new_points$lat, random_latlong))) # apply jitter to latitude
  if(length(new_long) > 0 & length(new_lat) > 0){ # do the rest only if there are coordinates to add
    new_coords <- as.data.frame(cbind(new_long, new_lat)) # store new coordinates
    names(new_coords) <- c("long", "lat") # change the column names for easy referencing
    taxon_name <- rep(taxon, nrow(new_coords))
    
    #sanity check plots
    # points should evenly cover only the portion of the range within USA
    #plot(raster_nam)
    #points(new_coords, pch=16, size=1)
    
    # store values for writing
    genus <- c(genus, rep(sapply(strsplit(taxon,"_"), `[`, 1), nrow(new_coords))) # split genus name
    species <- c(species, rep(sapply(strsplit(taxon,"_"), `[`, 2), nrow(new_coords))) # split species name 
    name <- c(name, taxon_name)
    new_long_list <- c(new_long_list, new_coords$long)
    new_lat_list <- c(new_lat_list, new_coords$lat)
  }
}

  ## Write out new simulated coordinates
  occurrence_ids <- seq(1, length(new_lat_list), 1) # create unique ids for each simulated occurrence
  new_lat_long_out <- data.frame(occurrence_ids, genus, species, name, new_long_list, new_lat_list)
  names(new_lat_long_out) <- c("occurrence_id", "genus", "species", "name", "longitude", "latitude")
  write.csv(new_lat_long_out,sub(".csv", "_newLatLongs_2021-01-13.csv",sub("input","output",input_csv_location)), row.names = FALSE) # write out results
```

